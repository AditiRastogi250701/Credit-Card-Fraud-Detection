# -*- coding: utf-8 -*-
"""Credit Card Fraud Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MFmRYviA6WjwCVJsNN4B8pGOG0lasPcG
"""

#importing

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#loading the dataset
dataset=pd.read_csv(r'/content/dataset.csv')

dataset.shape

dataset.isnull().sum()

for col in dataset.columns:
  print(col)

'''
for col in dataset.columns:
  if(dataset[col].isnull().sum()!=0):
    dataset[col]=dataset[col].fillna(dataset[col].mode(), inplace=True)


for column in dataset:
    if dataset[column].isnull().any():
      dataset[column]=dataset[column].fillna(dataset[column].mode(), inplace=True)
'''

'''
dataset['V7'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V8'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V9'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V10'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V11'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V12'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V13'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V14'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V15'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V16'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V17'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V18'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V19'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V20'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V21'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V22'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V23'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V24'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V25'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V26'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V27'].fillna(dataset['V7'].mode(), inplace=True)
dataset['V28'].fillna(dataset['V7'].mode(), inplace=True)
dataset['Amount'].fillna(dataset['V7'].mode(), inplace=True)
dataset['Class'].fillna(dataset['V7'].mode(), inplace=True)
'''

#dataset.replace(to_replace = np.nan, value = 0) 

dataset=dataset.dropna()

dataset.shape

dataset.head()

dataset.describe()

classes=pd.value_counts(dataset['Class'],sort=True)
classes.plot(kind='bar',rot=0)
plt.title("Transaction Distribution")
LABELS=['Normal','Fraud']
plt.xticks(range(2), LABELS)
plt.xlabel("Class")
plt.ylabel("Frequency")

amt_normal=dataset[dataset['Class']==0]
amt_fraud=dataset[dataset['Class']==1]

amt_normal.shape

amt_fraud.shape

f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)
f.suptitle('Amount per transaction by class')
bins = 50
ax1.hist(amt_normal.Amount, bins = bins)
ax1.set_title('Normal Transaction')
ax2.hist(amt_fraud.Amount, bins = bins)
ax2.set_title('Fraudulent Transaction')
plt.xlabel('Amount ($)')
plt.ylabel('Number of Transactions')
plt.yscale('log')
plt.show();

ax = amt_fraud.plot.scatter(x='Amount', y='Class', color='Orange', label='Fraud')
amt_normal.plot.scatter(x='Amount', y='Class', color='Blue', label='Normal', ax=ax)
plt.show()

x=dataset.drop(['Class'],axis=1)

y=dataset['Class']

x.shape

y.shape

#import standard scaler
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaler.fit(x)

scaler.transform(x)

x.head()

"""##Models"""

from sklearn import metrics
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

from sklearn import metrics
from sklearn.metrics import accuracy_score

from pandas.core.common import random_state
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2)

x_train.shape

y_train.shape

x_test.shape

y_test.shape

"""###Logistic Regression"""

lr=LogisticRegression()

lr.fit(x_train,y_train)

"""This means that the training dataset is too big to give an optimized solution. Hence taking a part of the dataset."""

data=dataset.sample(frac = 0.1,random_state=1)

data.shape

data.head()

x=data.drop(['Class'],axis=1)

y=data['Class']

x.shape

y.shape

x_train,x_test,y_train,y_test=train_test_split(x, y, test_size=0.2)

print("Shape of x_train", x_train.shape)
print("Shape of y_train", y_train.shape)
print("Shape of x_test", x_test.shape)
print("Shape of y_test", y_test.shape)

"""##Models

###Decision Tree Classifier
"""

dt=DecisionTreeClassifier(max_depth = 3, random_state = 1)
dt.fit(x_train,y_train)

x_train_result=dt.predict(x_train)

train_accuracy=accuracy_score(y_train,x_train_result)

train_accuracy

x_test_result=dt.predict(x_test)

x_test_result

test_accuracy_dt=accuracy_score(y_test,x_test_result)

test_accuracy_dt

from sklearn import tree

model=DecisionTreeClassifier(max_depth = 3, random_state = 1)

model.fit(x_train,y_train)

model.predict(x_test)

tree.plot_tree(model,filled=True)

"""###KNN-Classifier"""

knn=KNeighborsClassifier(n_neighbors=3)

knn.fit(x_train,y_train)

x_train_result=knn.predict(x_train)

train_accuracy=accuracy_score(x_train_result,y_train)

train_accuracy

x_test_result=knn.predict(x_test)

test_accuracy_knn=accuracy_score(x_test_result,y_test)

test_accuracy_knn

"""###SVC"""

from sklearn import svm

svc=svm.SVC(kernel='linear')

svc.fit(x_train,y_train)

x_train_result=svc.predict(x_train)

train_accuracy=accuracy_score(x_train_result,y_train)

x_test_result=svc.predict(x_test)

test_accuracy_svc=accuracy_score(x_test_result,y_test)

train_accuracy

test_accuracy_svc

"""###Random Forest Classifier"""

rfc=RandomForestClassifier(n_estimators = 100)

rfc.fit(x_train,y_train)

train_result=rfc.predict(x_train)

train_accuracy=accuracy_score(train_result,y_train)

train_accuracy

test_result=rfc.predict(x_test)

test_accuracy_rfc=accuracy_score(test_result,y_test)

test_accuracy_rfc

"""##Deep Neural Networks

###Artificial Neural Nets (ANN)
"""

import tensorflow as tf
from tensorflow import keras

model = keras.Sequential([
  keras.layers.Dense(input_dim = 30, units = 16, activation = 'relu'),
  keras.layers.Dense(units = 24, activation='relu'),
  keras.layers.Dropout(0.5),
  keras.layers.Dense(units = 20, activation = 'relu'),
  keras.layers.Dense(units = 24, activation = 'relu'),
  keras.layers.Dense(units =1, activation = 'sigmoid'),
])

model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
model.fit(x_train, y_train, batch_size = 15, epochs = 5)

loss,score_ann=model.evaluate(x_test,y_test)

score_ann

"""###Convolutional Neural Network (CNN)"""

from tensorflow.keras import datasets, layers, models
from keras.utils import np_utils

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Flatten,Conv1D,BatchNormalization,Dropout

x_train.shape

x_train

x_train=scaler.fit_transform(x_train)

x_train

x_test=scaler.fit_transform(x_test)

print(x_train.shape)
print(x_test.shape)

x_train=x_train.reshape(x_train.shape[0],x_train.shape[1],1)
x_test=x_test.reshape(x_test.shape[0],x_test.shape[1],1)

model=Sequential()
model.add(Conv1D(filters=32, kernel_size=2, activation='relu', input_shape=x_train[0].shape))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation='relu'))

y_train=y_train.to_numpy()
y_test=y_test.to_numpy()

model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
model.fit(x_train, y_train, batch_size = 15, epochs = 5)

loss,score_cnn=model.evaluate(x_test,y_test)

score_cnn

history=model.fit(x_train,y_train,epochs=10,validation_data=(x_test,y_test))

def plot_learningcurve(history,epochs):
  epoch=range(1,epochs+1)
  # accuracy
  plt.plot(epoch, history.history['accuracy'])
  plt.plot(epoch, history.history['val_accuracy'])
  plt.title('Model accuracy')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.legend(['train','val'], loc='upper left')
  plt.show()

  # loss
  plt.plot(epoch, history.history['loss'])
  plt.plot(epoch, history.history['val_loss'])
  plt.title('Model loss')
  plt.xlabel('epoch')
  plt.ylabel('loss')
  plt.legend(['train','val'], loc='upper left')
  plt.show()

plot_learningcurve(history,10)

accuracies=[test_accuracy_dt,test_accuracy_knn,test_accuracy_rfc,test_accuracy_svc,score_ann,score_cnn]
fig=plt.figure()
ax = fig.add_axes([0.1, 0.1, 0.8, 0.8]) # main axes
plt.plot(accuracies)
plt.title("Accuracies")
plt.xlabel("Algorithms")
plt.ylabel("Accuracy Score")
ax.set_xticklabels(['DTC','KNN','RFC','SVC','ANN','CNN'])
plt.show()

print(accuracies)

i=(np.argmax(accuracies))
names=['Decision Tree Classifier','K-Nearest Neighbor','Random Forest Classifier','Support Vector Classifier','Artificial Neural Networks','Convolutional Neural Networks']
print("Max accuracy is given by '{}' - {}%".format(names[i],round((100*accuracies[i]),2)))